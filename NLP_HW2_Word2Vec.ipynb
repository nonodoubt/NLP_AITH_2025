{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Импорты и переменные"
      ],
      "metadata": {
        "id": "ca5xsKpzrENH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install corus\n",
        "!pip install pymorphy3\n",
        "!pip install gensim\n",
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
        "!pip install navec\n",
        "!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar #модель navec\n",
        "!wget https://vectors.nlpl.eu/repository/20/184.zip #модель rusvector\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import pymorphy3\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from corus import load_lenta\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from navec import Navec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Загружаем стоп-слова\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "# Загружаем данные\n",
        "path = 'lenta-ru-news.csv.gz'\n",
        "records = load_lenta(path)\n",
        "\n",
        "max_samples = 10000 # обрежем датасет до 100000 записей"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvLl6WvJ_17G",
        "outputId": "387550ce-4162-4555-ea40-723528ca8c88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: corus in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "--2025-03-14 19:44:50--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250314%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250314T194450Z&X-Amz-Expires=300&X-Amz-Signature=e770db4963db42070d938ba85476fa95459e9c45a485ba5281f1b13ce3f37c59&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-14 19:44:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250314%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250314T194450Z&X-Amz-Expires=300&X-Amz-Signature=e770db4963db42070d938ba85476fa95459e9c45a485ba5281f1b13ce3f37c59&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 527373240 (503M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.gz.2’\n",
            "\n",
            "lenta-ru-news.csv.g 100%[===================>] 502.94M  62.4MB/s    in 8.3s    \n",
            "\n",
            "2025-03-14 19:44:59 (60.6 MB/s) - ‘lenta-ru-news.csv.gz.2’ saved [527373240/527373240]\n",
            "\n",
            "Requirement already satisfied: navec in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from navec) (1.26.4)\n",
            "--2025-03-14 19:45:02--  https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar\n",
            "Resolving storage.yandexcloud.net (storage.yandexcloud.net)... 213.180.193.243, 2a02:6b8::1d9\n",
            "Connecting to storage.yandexcloud.net (storage.yandexcloud.net)|213.180.193.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53012480 (51M) [application/x-tar]\n",
            "Saving to: ‘navec_hudlit_v1_12B_500K_300d_100q.tar.12’\n",
            "\n",
            "navec_hudlit_v1_12B 100%[===================>]  50.56M  14.0MB/s    in 4.8s    \n",
            "\n",
            "2025-03-14 19:45:08 (10.6 MB/s) - ‘navec_hudlit_v1_12B_500K_300d_100q.tar.12’ saved [53012480/53012480]\n",
            "\n",
            "--2025-03-14 19:45:08--  https://vectors.nlpl.eu/repository/20/184.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.200, 2001:700:112::200\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 640196018 (611M) [application/zip]\n",
            "Saving to: ‘184.zip.2’\n",
            "\n",
            "184.zip.2           100%[===================>] 610.54M  13.2MB/s    in 48s     \n",
            "\n",
            "2025-03-14 19:45:56 (12.6 MB/s) - ‘184.zip.2’ saved [640196018/640196018]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Лемматизация и предобработка текста"
      ],
      "metadata": {
        "id": "PZAbWelarLW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Приводим к нижнему регистру\n",
        "    text = re.sub(r'\\d+', '', text)  # Убираем числа\n",
        "    text = re.sub(r'[^а-яА-Яa-zA-Z\\s]', '', text)  # Убираем знаки препинания\n",
        "    words = text.split()  # Токенизация\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word not in stop_words]  # Лемматизация\n",
        "    return words #word2vecпринимает данные в виде списка предложений, где каждое предложение — это список слов\n",
        "\n",
        "# Преобразуем записи в DataFrame\n",
        "data = []\n",
        "for record in tqdm(records, desc=\"Загрузка данных\"):\n",
        "    data.append({\n",
        "        \"title\": record.title,\n",
        "        \"text\": record.text,\n",
        "        \"topic\": record.topic.strip()  # Убираем лишние пробелы\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Фильтрация классов: удаляем те, где менее 100 объектов класса\n",
        "class_counts = df['topic'].value_counts()\n",
        "print(f\"Число строк до фильтрации: {len(df)}\")\n",
        "print(df['topic'].value_counts())\n",
        "valid_classes = class_counts[class_counts >= 100].index\n",
        "df_filtered = df[df['topic'].isin(valid_classes)]\n",
        "print()\n",
        "\n",
        "# Ограничиваем размер выборки до 100000\n",
        "if len(df_filtered) > max_samples:\n",
        "    df_filtered = df_filtered.sample(n=max_samples, random_state=42)\n",
        "\n",
        "# После выборки проверяем классы заново\n",
        "class_counts_after = df_filtered['topic'].value_counts()\n",
        "valid_classes_after = class_counts_after[class_counts_after >= 100].index\n",
        "df_final = df_filtered[df_filtered['topic'].isin(valid_classes_after)]\n",
        "\n",
        "# Если после фильтрации строк стало меньше 10,000 — дополняем их из самых популярных тем\n",
        "if len(df_final) < max_samples:\n",
        "    remaining_samples = max_samples - len(df_final)\n",
        "    most_common_topic = df_filtered['topic'].value_counts().idxmax()\n",
        "    additional_samples = df_filtered[df_filtered['topic'] == most_common_topic].sample(n=remaining_samples, random_state=42)\n",
        "    df_final = pd.concat([df_final, additional_samples])\n",
        "\n",
        "# Итоговый датасет\n",
        "print(f\"Число строк после финальной фильтрации: {len(df_final)}\")\n",
        "print(df_final['topic'].value_counts())\n",
        "\n",
        "# Применяем предобработку текста\n",
        "tqdm.pandas(desc=\"Предобработка текста\")\n",
        "df_final['processed_text'] = df_final['text'].progress_apply(preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1J8HGjrr1Pt",
        "outputId": "77fa8ec5-ae12-49fa-e980-1eab029d5c38"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Загрузка данных: 739351it [01:17, 9531.96it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Число строк до фильтрации: 739351\n",
            "topic\n",
            "Россия               160519\n",
            "Мир                  136680\n",
            "Экономика             79538\n",
            "Спорт                 64421\n",
            "Культура              53803\n",
            "Бывший СССР           53402\n",
            "Наука и техника       53136\n",
            "Интернет и СМИ        44675\n",
            "Из жизни              27611\n",
            "Дом                   21734\n",
            "Силовые структуры     19596\n",
            "Ценности               7766\n",
            "Бизнес                 7399\n",
            "Путешествия            6408\n",
            "69-я параллель         1268\n",
            "Крым                    666\n",
            "Культпросвет            340\n",
            "                        203\n",
            "Легпром                 114\n",
            "Библиотека               65\n",
            "Оружие                    3\n",
            "ЧМ-2014                   2\n",
            "МедНовости                1\n",
            "Сочи                      1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Число строк после финальной фильтрации: 10000\n",
            "topic\n",
            "Россия               2407\n",
            "Мир                  1802\n",
            "Экономика            1045\n",
            "Спорт                 873\n",
            "Бывший СССР           748\n",
            "Культура              729\n",
            "Наука и техника       701\n",
            "Интернет и СМИ        572\n",
            "Из жизни              329\n",
            "Дом                   327\n",
            "Силовые структуры     255\n",
            "Ценности              109\n",
            "Путешествия           103\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Предобработка текста: 100%|██████████| 10000/10000 [02:59<00:00, 55.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка выборок"
      ],
      "metadata": {
        "id": "dljfVnPhtCvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_final['processed_text']\n",
        "y = df_final['topic']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=222)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=222)"
      ],
      "metadata": {
        "id": "q1x_ViPFr_5i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка word2vec эмбеддингов"
      ],
      "metadata": {
        "id": "OC9YE8oBsFuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение моделей и подбор гиперпараметров производится на 3 моделях:\n",
        "\n",
        "* Word2Vec\n",
        "* Navec\n",
        "* Rusvectores"
      ],
      "metadata": {
        "id": "_p0yo6i-1mqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "YjQvhpJgw4WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
      ],
      "metadata": {
        "id": "pPTOxQImsIVM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_vocab = list(w2v_model.wv.key_to_index.keys())\n",
        "print(words_in_vocab[:10])  # Показываем только первые 10 слов\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_sEVUR3RFdE",
        "outputId": "fae3d5f1-6b16-4db2-93e6-9746a6c91b1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['год', 'который', 'россия', 'сообщать', 'также', 'человек', 'время', 'свой', 'заявить', 'процент']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Оценка качества эмбеддингов\n",
        "print(\"Пример слов, похожих на 'машина':\", w2v_model.wv.most_similar('человек', topn=10))\n",
        "print(\"Слово, которое не подходит к остальным: \", w2v_model.wv.doesnt_match(['автомобиль', 'транспорт', 'дерево', 'машина']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMrGPIQxA905",
        "outputId": "d0e86a7b-4b8e-4f71-beca-a52ab5889661"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пример слов, похожих на 'машина': [('семья', 0.8197800517082214), ('сириец', 0.7667872309684753), ('усыновить', 0.709941565990448), ('ребёнок', 0.7098533511161804), ('проживать', 0.686340868473053), ('виновныхв', 0.6855184435844421), ('ранение', 0.6844428777694702), ('пассажир', 0.6753224730491638), ('военнослужащий', 0.6736987829208374), ('родственник', 0.6706948280334473)]\n",
            "Слово, которое не подходит к остальным:  транспорт\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Navec"
      ],
      "metadata": {
        "id": "U0kVnU4rz5qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "navec_model = Navec.load(path)\n",
        "\n",
        "word = 'россия'\n",
        "vector = navec_model[word]  # Получаем вектор для слова 'россия'\n",
        "print(f\"Вектор для слова '{word}':\", vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq71cXoPsQi3",
        "outputId": "42d6287f-26b2-4a4c-a054-629dbd525f2a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вектор для слова 'россия': [ 0.22543699 -0.39721358  0.6805563   0.21706595 -0.19716908 -0.20722607\n",
            " -0.07350219  0.13129961 -0.17141329  0.09088685  0.21599719 -0.09282316\n",
            "  0.00766279 -0.11043157 -0.07346303  0.42286018 -0.26629096  0.31371886\n",
            " -0.08937341  0.09485467 -0.04480258 -0.44643393 -0.3061798  -0.2882515\n",
            "  0.5377174   0.36234093  0.0030303   0.23453966 -0.28672412 -0.20668298\n",
            " -0.19193137  0.04902396  0.8125157   0.5318507  -0.6188356  -0.04572238\n",
            " -0.02173791 -0.66719943 -0.7230108  -0.2762196  -0.23562106  0.5413357\n",
            " -0.05294172  0.6201654  -0.8374897  -0.36382714  0.4649254  -0.13510832\n",
            "  0.09727751 -0.10602053  0.37899598 -0.36541265 -0.20060915  0.10681065\n",
            " -0.5519943  -0.13753682 -0.01502502  0.09474958 -0.05980838 -0.02857767\n",
            " -0.55855787 -0.04823827 -0.3578416   0.88438463  0.32023084 -0.25467572\n",
            "  0.22748815 -0.6873215  -0.04857488 -0.7444249  -0.41156083  0.19128552\n",
            "  0.07123397 -0.14737812 -0.39385885 -0.24628565  0.5453377  -0.05320076\n",
            "  0.06201499 -0.04103868 -0.2932616   0.3779095  -0.31397003 -0.04615191\n",
            " -0.43017006 -0.00623761  0.61113805  0.2079509  -0.05063467  0.09398386\n",
            " -0.41160762  0.09362367 -0.8432128  -0.10191941  0.01678637  0.6130639\n",
            " -0.50620526  0.06421215  0.15180896 -1.001532   -0.29364854 -0.1847418\n",
            "  0.05156467 -0.23999164  0.49402934  0.34105918 -0.8930306  -0.09359165\n",
            "  0.5060184   0.56454605 -0.21829388  0.05214137 -0.5776496   0.13659137\n",
            " -0.3143733   0.12403905  0.11754724  0.09411987  0.27510226 -0.05934107\n",
            " -0.39489806 -0.2788272  -0.03585588  0.39080086  0.01645428  0.00155893\n",
            " -0.6858619   0.03199653 -0.23169942 -0.57183355  0.7449328  -0.2478331\n",
            " -0.0656902  -0.23359276 -0.15860626  0.46892813  0.25094634 -0.0038072\n",
            "  0.8120501  -0.3921491   0.53953594  0.0864343   0.16530593 -0.34294814\n",
            " -0.660825    0.17502032  0.1601436   0.12880398  0.2447404  -0.1398195\n",
            "  0.3940808  -0.20316295  0.40920585  0.30853412  0.24878557 -0.08787356\n",
            " -0.00201242 -0.0976093   0.24608812 -0.20107856 -0.08510465 -0.22498815\n",
            " -0.24412297  0.11659407 -0.23810975  0.7927576  -0.19332255  0.35179833\n",
            "  0.1266067  -0.22534452  0.22342923  0.52875555 -0.13713977  0.5322976\n",
            " -0.01586652 -0.36773518  0.18660183 -0.15345146 -0.6563604   0.03934827\n",
            " -0.2537932   0.5915523   0.17447926  0.08440425 -0.14555511  0.8462292\n",
            "  0.02306626 -0.50092953  0.02497473  0.09709087  0.42852387 -0.12507445\n",
            " -0.43765643 -0.04486881 -0.9197812   0.15333776  0.21755728  0.16034487\n",
            "  0.10947693  0.26935494  0.08937906 -0.32551923  0.30181772  0.03085733\n",
            "  0.32175773  0.58648676 -0.4552861  -0.09058901 -0.07721699  0.13540809\n",
            " -0.00645914 -0.24767125 -0.17353976 -0.32662928  0.55255526 -0.31210357\n",
            "  0.71981466 -0.35557505 -0.2027955   0.5551026  -0.30438095 -0.41725138\n",
            " -0.4683033   0.03520601  0.67806554 -0.02468547  0.33984688  0.43193454\n",
            " -0.27096793 -0.02520603  0.286634   -0.02133529 -0.55753946  0.2741532\n",
            "  0.21432668 -0.4251435   0.3402687   0.3789248   0.38285363 -0.01478797\n",
            "  0.4644731   0.21247226 -0.6237922  -0.14217526 -0.36516917  0.14149092\n",
            " -0.15003966  0.07534812 -0.26868176  0.23520207 -0.54361004 -0.9098788\n",
            " -0.3157734   0.18669239 -0.0962809  -0.37809882  0.4742351   0.13234845\n",
            " -0.50550336  0.17703918 -0.44975471  0.19239506  0.30779085  0.24575497\n",
            "  0.5954058   0.45039576 -0.16555037  0.11469514 -0.0700791   0.00519014\n",
            "  0.64192    -0.07611681 -0.04211494 -0.04530421 -0.69490165 -0.5775567\n",
            " -0.34478965 -0.05541408  0.5040086   0.14443237 -0.49850866  0.23184095\n",
            "  0.10807174 -0.06104805  0.24632384  0.4157553  -0.3786785  -0.18116818\n",
            "  0.5042701   0.6235093   0.28725666 -0.41006738 -0.27313936 -0.1506457\n",
            " -0.24990621 -0.3762372   0.15003653 -0.05707216  0.10502052  0.30300924]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rusvectors"
      ],
      "metadata": {
        "id": "_oR9VEaTuzUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Распаковываем архив\n",
        "zip_path = '184.zip'\n",
        "output_dir = '/content/models/'  # Папка для распаковки\n",
        "\n",
        "# Распаковываем архив\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "# Проверяем, что было распаковано\n",
        "print(\"Файлы в распакованном архиве:\", os.listdir(output_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0BwUHjSgc4J",
        "outputId": "11d5a53e-3591-4522-ea96-1e396c2c8132"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файлы в распакованном архиве: ['model.bin', 'news_upos_cbow_600_2_2018.vec', 'README', 'model.hdf5', 'vocab.txt', 'meta.json', 'options.json', 'model.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Путь к распакованной модели (предположим, что это файл .vec)\n",
        "model_path = os.path.join(output_dir, 'model.txt')\n",
        "\n",
        "# Загрузка модели (для текстового формата используем binary=False)\n",
        "rusvectores_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)\n"
      ],
      "metadata": {
        "id": "FUnD31YUu4dL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования модели\n",
        "word = 'машина_NOUN'\n",
        "vector = rusvectores_model[word]  # Получаем вектор для слова\n",
        "print(f\"Вектор для слова '{word}':\", vector)\n",
        "\n",
        "# Пример поиска похожих слов\n",
        "similar_words = rusvectores_model.most_similar(word, topn=10)\n",
        "print(f\"Похожие слова для '{word}':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df37bisyt2XW",
        "outputId": "217c7b2a-e999-4f3c-a65a-e6c2b33426b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вектор для слова 'машина_NOUN': [-6.16980083e-02  1.38541609e-01  1.00030132e-01  2.34297980e-02\n",
            "  1.11793719e-01  2.28444517e-01 -2.10410625e-01  1.63041517e-01\n",
            "  1.77408546e-01 -4.21602756e-01 -2.47368976e-01 -2.04373840e-02\n",
            " -4.40045178e-01 -1.93381414e-01 -4.40929830e-03  5.53002581e-02\n",
            " -2.66674191e-01  6.14620447e-02 -6.63286895e-02  2.14453697e-01\n",
            "  2.31444240e-01 -6.41284958e-02  4.39997405e-01  2.50135422e-01\n",
            " -7.03658089e-02 -2.60625500e-02 -8.10098723e-02 -3.93783003e-02\n",
            " -1.80126771e-01  2.61675864e-01 -2.92723119e-01 -1.32760048e-01\n",
            " -9.94674563e-02 -4.11767215e-02  1.28507882e-01  1.84862703e-01\n",
            " -1.20293371e-01 -7.81306326e-02  1.52414709e-01 -3.34452301e-01\n",
            " -1.90250993e-01 -1.96084231e-01  5.41896112e-02  2.50238508e-01\n",
            "  1.76131621e-01  3.68228436e-01 -9.92657244e-02 -1.66030508e-02\n",
            "  1.06333375e-01  4.77112792e-02 -5.73637962e-01  1.14929825e-01\n",
            "  4.82819676e-02 -2.55000800e-01 -2.35229015e-01 -6.26579374e-02\n",
            "  6.29792316e-03 -5.20189367e-02  1.06577158e-01 -3.62688266e-02\n",
            " -8.86167511e-02 -4.24625814e-01 -2.87861019e-01  2.74608999e-01\n",
            " -3.07049870e-01  4.62724477e-01 -9.30594932e-03 -3.06278802e-02\n",
            " -2.77535826e-01 -5.40083535e-02  2.88721681e-01  2.47172430e-01\n",
            " -1.49828732e-01 -2.89138388e-02 -1.85949877e-01 -2.41994664e-01\n",
            " -1.85686857e-01 -1.44057438e-01  1.75929144e-01 -3.05042058e-01\n",
            "  4.23586160e-01  9.02932882e-02  3.11673373e-01  3.52972388e-01\n",
            "  4.04241771e-01 -3.22130263e-01 -2.10021436e-01 -4.23642039e-01\n",
            " -1.46653503e-01  1.61850646e-01  1.79529026e-01 -1.16966203e-01\n",
            " -3.98756832e-01  1.96873713e-02 -1.98457956e-01  3.23750436e-01\n",
            " -1.28539586e-02  1.50703803e-01  5.42111360e-02 -1.10683806e-01\n",
            " -1.79337069e-01 -2.84766346e-01  5.99700324e-02  1.39725626e-01\n",
            "  7.82034174e-02 -4.09592837e-02 -3.29018354e-01  2.48698875e-01\n",
            "  1.15852989e-01  2.74796516e-01 -1.55914068e-01 -9.93354097e-02\n",
            " -9.29897055e-02  7.70524144e-02 -1.34647163e-02 -3.77106577e-01\n",
            " -9.83693078e-02  1.27372742e-01 -2.81047616e-02  9.27400440e-02\n",
            " -1.26524955e-01 -1.37635618e-01 -8.30407813e-02 -2.03517303e-01\n",
            "  6.46983236e-02  1.18019558e-01  2.05444396e-01 -3.30834091e-01\n",
            "  1.81481257e-01  4.03360516e-01  9.59431902e-02 -1.72209606e-01\n",
            " -1.05701797e-01 -1.56791955e-01 -1.28396675e-01 -3.45016941e-02\n",
            " -2.14445498e-03 -7.36101717e-02 -2.39228934e-01  3.56666028e-01\n",
            " -2.51537442e-01  4.34445962e-02  2.10081283e-02 -1.06877916e-01\n",
            " -6.32366091e-02  1.08065136e-01  8.12347829e-02 -1.31056458e-01\n",
            " -1.37807474e-01 -2.75581062e-01 -7.00250864e-02  2.06151053e-01\n",
            " -2.52618551e-01 -2.78766990e-01  1.17237397e-01  4.41200137e-02\n",
            "  1.92486301e-01  5.92394210e-02  1.43181875e-01 -1.33190244e-01\n",
            "  2.66283542e-01  3.37162495e-01 -3.45633030e-01  1.89481273e-01\n",
            " -3.52370799e-01 -3.02164294e-02 -9.17852297e-02  2.02253088e-01\n",
            "  1.78947579e-02  1.55275226e-01  5.61034270e-02  1.78972587e-01\n",
            "  1.72666043e-01 -2.27210388e-01 -1.80963531e-01 -1.76361069e-01\n",
            "  2.79626042e-01  1.63398668e-01 -8.82140398e-02  4.57005411e-01\n",
            "  2.73766518e-01 -4.09030050e-01 -2.28115857e-01 -1.33941755e-01\n",
            "  8.74055848e-02  1.67915940e-01  1.00238763e-01 -3.57025951e-01\n",
            " -4.32389081e-01 -3.40977423e-02 -1.98291466e-01  2.11169291e-02\n",
            "  1.53934672e-01  2.19367314e-02 -1.94554757e-02 -5.31177968e-02\n",
            "  6.36749947e-03  9.55820382e-02 -3.06021214e-01  1.68664172e-01\n",
            "  2.55217813e-02 -2.62878716e-01  6.13387451e-02  4.00238633e-01\n",
            " -6.00832820e-01  1.61521628e-01  1.46603748e-01 -2.46134460e-01\n",
            "  8.68247896e-02 -2.21379682e-01 -2.58170456e-01 -6.87483326e-02\n",
            "  3.20888981e-02 -1.24788783e-01  5.53804450e-02  1.72412142e-01\n",
            " -4.70896661e-01  3.50359291e-01  6.70050308e-02 -6.04503453e-01\n",
            "  4.39533265e-03  3.39715965e-02  4.55696844e-02 -3.27862591e-01\n",
            " -7.69194886e-02  2.22904295e-01 -4.61830616e-01  6.27193898e-02\n",
            "  3.18543553e-01 -1.69010758e-01  1.56665891e-01 -1.85562447e-01\n",
            " -1.01705380e-01 -2.04312503e-01 -4.27552462e-02 -8.99335071e-02\n",
            " -4.80584592e-01  2.88966477e-01  2.55929053e-01  1.54071329e-02\n",
            "  3.32878053e-01 -1.69320256e-01  5.21671116e-01  4.79938984e-01\n",
            "  6.19457709e-03  1.38323814e-01 -1.52743250e-01 -2.99383664e-05\n",
            " -6.23634085e-02  1.74046382e-02  4.28363085e-02  4.36123192e-01\n",
            " -1.82960197e-01  4.43913192e-02 -5.45378327e-01 -1.47459626e-01\n",
            "  1.49199665e-01  3.52012068e-02 -1.89646166e-02 -3.16202939e-01\n",
            " -1.54793218e-01 -1.68009043e-01  1.03498831e-01  7.09000602e-02\n",
            "  7.81837776e-02  2.12286860e-01  2.35409979e-02  3.97311663e-03\n",
            "  2.21490517e-01 -1.54734269e-01 -2.88778991e-02 -1.34935036e-01\n",
            "  1.97657183e-01 -7.70083815e-03  1.94226742e-01  2.30585515e-01\n",
            "  3.09007257e-01 -8.85055587e-02 -1.42173201e-01 -7.42088482e-02\n",
            "  1.85257394e-03 -2.11130992e-01 -1.02339208e-01  1.65856421e-01\n",
            "  2.21614540e-01 -6.21717386e-02 -1.29937055e-02 -2.27769747e-01\n",
            "  2.66202837e-01  6.08044527e-02  2.40619093e-01 -1.59795403e-01\n",
            " -1.47022143e-01  5.45472577e-02 -1.08813867e-01  3.45347524e-01\n",
            " -3.11647922e-01  1.01531424e-01 -3.13566864e-01 -1.06827188e-02]\n",
            "Похожие слова для 'машина_NOUN': [('автомобиль_NOUN', 0.7304336428642273), ('бмв_PROPN', 0.6006490588188171), ('жигули_PROPN', 0.5957476496696472), ('легковой_ADJ', 0.5930621027946472), ('авто_NOUN', 0.5768054723739624), ('иномарка_NOUN', 0.5707587003707886), ('ауди_VERB', 0.5496861934661865), ('жигуленок_NOUN', 0.5436868071556091), ('куча-мал_ADJ', 0.5344013571739197), ('сугробый_NOUN', 0.5311343669891357)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Векторизация текстов с использованием w2v, navec и rusvectores"
      ],
      "metadata": {
        "id": "lrpjm6MADm9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_w2v_embedding_word2vec(text, model, vector_size=100):\n",
        "    '''Функция векторизации текста с использованием модели Word2Vec'''\n",
        "    embeddings = []\n",
        "    for word in text:\n",
        "        if word in model.wv:  # Для Word2Vec используем model.wv\n",
        "            embeddings.append(model.wv[word])  # Получаем вектор\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)  # Усредняем вектора\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Если нет векторов, возвращаем нулевой вектор\n"
      ],
      "metadata": {
        "id": "I6QSwYXxDTqG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_w2v_embedding_navec(text, model, vector_size=300):\n",
        "    '''Функция векторизации текста с использованием модели Navec'''\n",
        "    embeddings = []\n",
        "\n",
        "    for word in text:\n",
        "        if word in model:  # Для Navec используем прямой доступ через модель\n",
        "            embeddings.append(model[word])  # Получаем вектор для слова\n",
        "\n",
        "    # Если есть вектора, усредняем их\n",
        "    if embeddings:\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        # Проверяем, что все вектора имеют нужную размерность\n",
        "        if embeddings.shape[1] != vector_size:\n",
        "            embeddings = np.resize(embeddings, (embeddings.shape[0], vector_size))  # Подгоняем размерность вектора\n",
        "\n",
        "        # Усредняем вектора\n",
        "        return np.mean(embeddings, axis=0)  # Усредняем вектора\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Если нет векторов, возвращаем нулевой вектор\n",
        "\n"
      ],
      "metadata": {
        "id": "sPD5ghZK1krn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_w2v_embedding_rusvectores(text, model, vector_size=100):\n",
        "    '''Функция векторизации текста с использованием модели RusVectores'''\n",
        "    embeddings = []\n",
        "    for word in text:\n",
        "        if word in model:  # Для RusVectores используем прямой доступ через модель\n",
        "            embeddings.append(model[word])  # Получаем вектор для слова\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)  # Усредняем вектора\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Если нет векторов, возвращаем нулевой вектор\n"
      ],
      "metadata": {
        "id": "zYUS_zad1qyk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение функции для векторизации (для всех трех моделей)\n",
        "X_train_w2v = np.array([get_w2v_embedding_word2vec(text, w2v_model) for text in X_train])\n",
        "X_val_w2v = np.array([get_w2v_embedding_word2vec(text, w2v_model) for text in X_val])\n",
        "X_test_w2v = np.array([get_w2v_embedding_word2vec(text, w2v_model) for text in X_test])\n",
        "\n",
        "X_train_navec = np.array([get_w2v_embedding_navec(text, navec_model) for text in X_train])\n",
        "X_val_navec = np.array([get_w2v_embedding_navec(text, navec_model) for text in X_val])\n",
        "X_test_navec = np.array([get_w2v_embedding_navec(text, navec_model) for text in X_test])\n",
        "\n",
        "X_train_rusvectores = np.array([get_w2v_embedding_rusvectores(text, rusvectores_model) for text in X_train])\n",
        "X_val_rusvectores = np.array([get_w2v_embedding_rusvectores(text, rusvectores_model) for text in X_val])\n",
        "X_test_rusvectores = np.array([get_w2v_embedding_rusvectores(text, rusvectores_model) for text in X_test])\n"
      ],
      "metadata": {
        "id": "0rQn-UboDmCL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression с эмбеддингами"
      ],
      "metadata": {
        "id": "9REnZs260As3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Логистическая регрессия\n",
        "lr = LogisticRegression(max_iter=300, random_state=222)"
      ],
      "metadata": {
        "id": "FJbsJcNSEpSI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели для word2vec\n",
        "lr.fit(X_train_w2v, y_train)\n",
        "y_pred_w2v = lr.predict(X_val_w2v)\n",
        "print(\"Результаты для модели с word2vec:\")\n",
        "print(classification_report(y_val, y_pred_w2v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrNlFA7eDxjN",
        "outputId": "47806d5f-1f8e-4267-cfcb-6b697d7668f9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты для модели с word2vec:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "      Бывший СССР       0.54      0.20      0.29       150\n",
            "              Дом       0.77      0.62      0.69        66\n",
            "         Из жизни       0.36      0.20      0.25        66\n",
            "   Интернет и СМИ       0.48      0.35      0.40       114\n",
            "         Культура       0.56      0.63      0.60       145\n",
            "              Мир       0.62      0.76      0.69       361\n",
            "  Наука и техника       0.58      0.63      0.60       140\n",
            "      Путешествия       0.00      0.00      0.00        20\n",
            "           Россия       0.62      0.79      0.69       481\n",
            "Силовые структуры       0.00      0.00      0.00        51\n",
            "            Спорт       0.92      0.91      0.92       175\n",
            "         Ценности       1.00      0.09      0.17        22\n",
            "        Экономика       0.76      0.81      0.79       209\n",
            "\n",
            "         accuracy                           0.65      2000\n",
            "        macro avg       0.56      0.46      0.47      2000\n",
            "     weighted avg       0.62      0.65      0.62      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели для navec\n",
        "lr.fit(X_train_navec, y_train)\n",
        "y_pred_navec = lr.predict(X_val_navec)\n",
        "print(\"Результаты для модели с navec:\")\n",
        "print(classification_report(y_val, y_pred_navec))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUQWVM7vD0H4",
        "outputId": "bff7d9f0-17c6-4fec-d7c2-6c7b0aea7e51"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты для модели с navec:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "      Бывший СССР       0.74      0.61      0.67       150\n",
            "              Дом       0.85      0.71      0.78        66\n",
            "         Из жизни       0.46      0.44      0.45        66\n",
            "   Интернет и СМИ       0.68      0.62      0.65       114\n",
            "         Культура       0.83      0.81      0.82       145\n",
            "              Мир       0.72      0.78      0.75       361\n",
            "  Наука и техника       0.74      0.71      0.73       140\n",
            "      Путешествия       0.82      0.45      0.58        20\n",
            "           Россия       0.70      0.80      0.74       481\n",
            "Силовые структуры       0.38      0.10      0.16        51\n",
            "            Спорт       0.96      0.94      0.95       175\n",
            "         Ценности       0.76      0.59      0.67        22\n",
            "        Экономика       0.78      0.84      0.81       209\n",
            "\n",
            "         accuracy                           0.74      2000\n",
            "        macro avg       0.73      0.65      0.67      2000\n",
            "     weighted avg       0.74      0.74      0.74      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели для rusvectores\n",
        "lr.fit(X_train_rusvectores, y_train)\n",
        "y_pred_rusvectores = lr.predict(X_val_rusvectores)\n",
        "print(\"Результаты для модели с rusvectores:\")\n",
        "print(classification_report(y_val, y_pred_rusvectores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PljtVGp0D2aX",
        "outputId": "b6512fc2-6b2b-4eb3-a082-9062a4e6d65d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты для модели с rusvectores:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "      Бывший СССР       0.00      0.00      0.00       150\n",
            "              Дом       0.00      0.00      0.00        66\n",
            "         Из жизни       0.00      0.00      0.00        66\n",
            "   Интернет и СМИ       0.00      0.00      0.00       114\n",
            "         Культура       0.00      0.00      0.00       145\n",
            "              Мир       0.00      0.00      0.00       361\n",
            "  Наука и техника       0.00      0.00      0.00       140\n",
            "      Путешествия       0.00      0.00      0.00        20\n",
            "           Россия       0.24      1.00      0.39       481\n",
            "Силовые структуры       0.00      0.00      0.00        51\n",
            "            Спорт       0.00      0.00      0.00       175\n",
            "         Ценности       0.00      0.00      0.00        22\n",
            "        Экономика       0.00      0.00      0.00       209\n",
            "\n",
            "         accuracy                           0.24      2000\n",
            "        macro avg       0.02      0.08      0.03      2000\n",
            "     weighted avg       0.06      0.24      0.09      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression + Navec + с TfidfVectorizer"
      ],
      "metadata": {
        "id": "4IigHHD40Eq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для того чтобы улучшить качество модели, мы можем использовать взвешенные эмбеддинги. Это означает, что вместо того, чтобы просто усреднять все вектора слов в тексте, мы будем учитывать важность каждого слова с помощью веса tf-idf."
      ],
      "metadata": {
        "id": "RKO6WfcV8oMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразуем каждый список слов в строку\n",
        "X_train_str = [' '.join(text) for text in X_train]\n",
        "X_val_str = [' '.join(text) for text in X_val]\n",
        "X_test_str = [' '.join(text) for text in X_test]\n",
        "\n",
        "# Теперь применим TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words=list(stop_words))\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_str)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val_str)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_str)\n",
        "\n",
        "# Получаем словарь слов и их индексов\n",
        "tfidf_vocab = tfidf_vectorizer.get_feature_names_out()\n"
      ],
      "metadata": {
        "id": "jcBWvmuf8vxf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_w2v_embedding_navec_tfidf(text, model, tfidf_matrix, tfidf_vocab, vector_size=300):\n",
        "    '''Функция векторизации текста с использованием модели Navec и tf-idf весов'''\n",
        "    embeddings = []\n",
        "\n",
        "    for word in text:\n",
        "        if word in model:  # Для Navec используем прямой доступ через модель\n",
        "            word_index = tfidf_vocab.tolist().index(word) if word in tfidf_vocab else -1\n",
        "\n",
        "            if word_index != -1:\n",
        "                tfidf_weight = tfidf_matrix[0, word_index]  # Получаем вес слова из tf-idf\n",
        "                embeddings.append(model[word] * tfidf_weight)  # Взвешиваем вектор\n",
        "\n",
        "    # Усредняем вектора\n",
        "    if embeddings:\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        # Проверяем, что все вектора имеют нужную размерность\n",
        "        if embeddings.shape[1] != vector_size:\n",
        "            embeddings = np.resize(embeddings, (embeddings.shape[0], vector_size))  # Подгоняем размерность вектора\n",
        "\n",
        "        # Усредняем взвешенные вектора\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Если нет векторов, возвращаем нулевой вектор"
      ],
      "metadata": {
        "id": "Hx3FgkzSTlAj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_w2v_embedding_navec_tfidf(text, model, tfidf_matrix, tfidf_vocab, vector_size=300):\n",
        "    '''Функция векторизации текста с использованием модели Navec и tf-idf весов'''\n",
        "    embeddings = []\n",
        "\n",
        "    for word in text:\n",
        "        if word in model:  # Для Navec используем прямой доступ через модель\n",
        "            word_index = tfidf_vocab.tolist().index(word) if word in tfidf_vocab else -1\n",
        "\n",
        "            if word_index != -1:\n",
        "                tfidf_weight = tfidf_matrix[0, word_index]  # Получаем вес слова из tf-idf\n",
        "                embeddings.append(model[word] * tfidf_weight)  # Взвешиваем вектор\n",
        "\n",
        "    # Усредняем вектора\n",
        "    if embeddings:\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        # Проверяем, что все вектора имеют нужную размерность\n",
        "        if embeddings.shape[1] != vector_size:\n",
        "            embeddings = np.resize(embeddings, (embeddings.shape[0], vector_size))  # Подгоняем размерность вектора\n",
        "\n",
        "        # Усредняем взвешенные вектора\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Если нет векторов, возвращаем нулевой вектор"
      ],
      "metadata": {
        "id": "VsjrEmpbTnT0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применение взвешенной функции для векторизации\n",
        "# Преобразуем тексты в tf-idf матрицы\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_str)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val_str)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_str)\n",
        "\n",
        "# Получаем словарь слов и их индексов\n",
        "tfidf_vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Векторизация для Navec с tf-idf\n",
        "X_train_navec_tfidf = np.array([get_w2v_embedding_navec_tfidf(text, navec_model, X_train_tfidf, tfidf_vocab) for text in X_train])\n",
        "X_val_navec_tfidf = np.array([get_w2v_embedding_navec_tfidf(text, navec_model, X_val_tfidf, tfidf_vocab) for text in X_val])\n",
        "X_test_navec_tfidf = np.array([get_w2v_embedding_navec_tfidf(text, navec_model, X_test_tfidf, tfidf_vocab) for text in X_test])"
      ],
      "metadata": {
        "id": "SP1hWwVz-TIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(X_train, X_val, y_train, y_val):\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)  # Обучаем модель\n",
        "    y_pred_val = model.predict(X_val)  # Прогноз на валидационной выборке\n",
        "    return classification_report(y_val, y_pred_val)\n",
        "\n",
        "# Обучение и оценка для Navec с tf-idf\n",
        "print(\"Logistic Regression с Navec + tf-idf:\")\n",
        "report_navec_tfidf = train_and_evaluate(X_train_navec_tfidf, X_val_navec_tfidf, y_train, y_val)\n",
        "print(report_navec_tfidf)"
      ],
      "metadata": {
        "id": "NQb8O0G--ant"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выводы"
      ],
      "metadata": {
        "id": "FYe1gZmwOgm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод\n",
        "\n",
        "* Произведена подготовка датасета, включающая в себя очистку, лемматизацию, привдение к единообразию, токенизацию на список слов\n",
        "* Корпус текстов собран в датасет. Датасет разделен на тренеровочную, валидационную и тестовую выборки. Выборки созданы с учетом стратификации, редкие темы (меньше 65 объектов) отсеяны.\n",
        "* Обучены word2vec-эмбеддинги с помощью библиотеки gensim: word2vec, предобученная модель Navec, предобученная модель Rusvectors. Лучший результат на валидационной выборке с accuracy 0.74 показала модель Navec.\n",
        "* Для моделей произведено обучение и валидирование результатов\n",
        "* Выполнена доработка качества модели, взяв для ее обучения  набор эмбеддингов NAVEC, используя его с взвешиванием через tf-idf. После улучшения модель логистической регрессии показала результат ___________ (не успел досчитать)\n"
      ],
      "metadata": {
        "id": "KW2U4CajOge9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PlnPhOL9H6UX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}